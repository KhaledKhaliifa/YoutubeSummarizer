{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LLM Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5Tokenizer\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokenizer to calculate input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:240: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    task=\"summarization\",\n",
    "    model=model_name,\n",
    "    tokenizer= tokenizer,\n",
    "    model_kwargs={\"cache_dir\": '../models/'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', \n",
    "that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part \n",
    "of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \n",
    "training data, in order to make predictions or decisions without being explicitly programmed to do so. \n",
    "Machine learning algorithms are used in a wide variety of applications, such as in medicine, email \n",
    "filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop \n",
    "conventional algorithms to perform the needed tasks. Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks) \n",
    "to progressively extract higher-level features from raw input. For example, in image processing, lower layers may \n",
    "identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, \n",
    "and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, \n",
    "in its pursuit to fill the gap between human communication and computer understanding. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding \n",
    "from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the \n",
    "human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding \n",
    "digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic \n",
    "information, e.g., in the forms of decisions.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_tokens=128):\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    total_tokens = tokens.shape[1]\n",
    "\n",
    "    # If the text is less than the max_tokens, return the text as a single chunk\n",
    "    if total_tokens <= max_tokens:\n",
    "        return [text]\n",
    "\n",
    "    # Split into sentences (rough approximation)\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Add period back to sentence\n",
    "        sentence = sentence.strip() + '.'\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        sentence_length = sentence_tokens.shape[1]\n",
    "\n",
    "        # If adding this sentence would exceed max_tokens, start a new chunk\n",
    "        if current_length + sentence_length > max_tokens and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and Max length calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lengths( text):\n",
    "        \"\"\"\n",
    "        Calculate appropriate summary lengths based on input text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text to summarize\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (min_length, max_length) in tokens\n",
    "        \"\"\"\n",
    "        # Calculate tokenized length\n",
    "        tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        input_length = tokens.shape[1]\n",
    "        \n",
    "        # Calculate appropriate summary lengths\n",
    "        max_length = min(input_length // 2, 150)  # Half of input or 150 tokens, whichever is smaller\n",
    "        min_length = max(input_length // 4, 5)   # Quarter of input or 30 tokens, whichever is larger\n",
    "        \n",
    "        return min_length, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(chunk):\n",
    "    min_length, max_length = calculate_lengths(chunk)\n",
    "    summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn' it is seen as a part of artificial intelligence \n",
      "-  machine learning algorithms are used in a wide variety of applications \n",
      "-  it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks \n",
      "-  deep learning uses neural networks with multiple layers to extract higher-level features from raw input \n",
      "-  Natural language processing (NLP) is a branch of artificial intelligence \n",
      "-  it helps computers understand, interpret, and manipulate human language \n",
      "-  NLP draws from computer science and computational linguistics \n",
      "-  tasks include acquiring, processing, analyzing and understanding digital images \n",
      "-  they also include extraction of high-dimensional data from the\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(test_text)\n",
    "if len(chunks) == 1:\n",
    "    summary = summarize_chunk(chunks[0])\n",
    "else:\n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summary.append(summarize_chunk(chunk))\n",
    "    summary = ' '.join(summary)\n",
    "\n",
    "bullet_points = summary.split(\".\")\n",
    "for point in bullet_points:\n",
    "    print(f\"- {point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', \n",
      "that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part \n",
      "of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \n",
      "training data, in order to make predictions or decisions without being explicitly programmed to do so.\n",
      "--------------------------------\n",
      "Machine learning algorithms are used in a wide variety of applications, such as in medicine, email \n",
      "filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop \n",
      "conventional algorithms to perform the needed tasks. Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks) \n",
      "to progressively extract higher-level features from raw input. For example, in image processing, lower layers may \n",
      "identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.\n",
      "--------------------------------\n",
      "Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, \n",
      "and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, \n",
      "in its pursuit to fill the gap between human communication and computer understanding. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding \n",
      "from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the \n",
      "human visual system can do.\n",
      "--------------------------------\n",
      "Computer vision tasks include methods for acquiring, processing, analyzing and understanding \n",
      "digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic \n",
      "information, e. g. , in the forms of decisions. .\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"--------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
