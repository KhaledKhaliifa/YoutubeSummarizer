{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LLM Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5Tokenizer, BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokenizer to calculate input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\n",
    "        \"cache_dir\": '../models/'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"\n",
    "Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', \n",
    "that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part \n",
    "of artificial intelligence. Machine learning algorithms build a model based on sample data, known as \n",
    "training data, in order to make predictions or decisions without being explicitly programmed to do so. \n",
    "Machine learning algorithms are used in a wide variety of applications, such as in medicine, email \n",
    "filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop \n",
    "conventional algorithms to perform the needed tasks. Deep learning is a subset of machine learning that uses neural networks with multiple layers (deep neural networks) \n",
    "to progressively extract higher-level features from raw input. For example, in image processing, lower layers may \n",
    "identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces. Natural Language Processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret, \n",
    "and manipulate human language. NLP draws from many disciplines, including computer science and computational linguistics, \n",
    "in its pursuit to fill the gap between human communication and computer understanding. Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding \n",
    "from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the \n",
    "human visual system can do. Computer vision tasks include methods for acquiring, processing, analyzing and understanding \n",
    "digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic \n",
    "information, e.g., in the forms of decisions.\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(text, max_tokens=128):\n",
    "    # Tokenize the entire text\n",
    "    tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    total_tokens = tokens.shape[1]\n",
    "\n",
    "    # If the text is less than the max_tokens, return the text as a single chunk\n",
    "    if total_tokens <= max_tokens:\n",
    "        return [text]\n",
    "\n",
    "    # Split into sentences (rough approximation)\n",
    "    sentences = text.split('.')\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Add period back to sentence\n",
    "        sentence = sentence.strip() + '.'\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "        sentence_length = sentence_tokens.shape[1]\n",
    "\n",
    "        # If adding this sentence would exceed max_tokens, start a new chunk\n",
    "        if current_length + sentence_length > max_tokens and current_chunk:\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min and Max length calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lengths( text):\n",
    "        # Calculate tokenized length\n",
    "        tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        input_length = tokens.shape[1]\n",
    "        \n",
    "        # Calculate appropriate summary lengths\n",
    "        max_length = min(input_length // 2, 150)  # Half of input or 150 tokens, whichever is smaller\n",
    "        min_length = max(input_length // 4, 5)   # Quarter of input or 30 tokens, whichever is larger\n",
    "        \n",
    "        return min_length, max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk(chunk):\n",
    "    min_length, max_length = calculate_lengths(chunk)\n",
    "    summary = summarizer(chunk, max_length=max_length, min_length=min_length, do_sample=False)[0]['summary_text']\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_text(test_text)\n",
    "if len(chunks) == 1:\n",
    "    summary = summarize_chunk(chunks[0])\n",
    "else:\n",
    "    summary = []\n",
    "    for chunk in chunks:\n",
    "        summary.append(summarize_chunk(chunk))\n",
    "    summary = ' '.join(summary)\n",
    "\n",
    "bullet_points = summary.split(\".\")\n",
    "for point in bullet_points:\n",
    "    print(f\"- {point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk)\n",
    "    print(\"--------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
